{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b5aca1a-ace7-43ce-906a-2137bf28a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import lftk\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('/data/pbcalder/home77/Documents/hierarchical-hawkes/on-VEVO/reduced_textvecs/my-32dim-model')\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121644a3-e966-4ec0-83ce-874a481dcf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8be14e-29e6-45af-8ff6-a6ca30012b63",
   "metadata": {},
   "source": [
    "# get vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6282aa0-4a8e-4755-a36a-edd6e9822f0c",
   "metadata": {},
   "source": [
    "## theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f5ae975-113c-4f17-971c-33a8cb7db738",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbcalder/anaconda3/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SimpleImputer from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/pbcalder/anaconda3/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/pbcalder/anaconda3/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator PCA from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#https://gist.github.com/bstonedev/9bc7f73001e24c5a73931f00955e0b1f\n",
    "fixer = {\"‚Äô\": \"'\",\n",
    "\"‚Äô\": \"'\",\n",
    "\"‚Äò\": \"'\",\n",
    "\"‚Äã\": \"\",\n",
    "\"‚Äã\": \"\",\n",
    "\"‚Ä¶\": \"...\",\n",
    "\"‚Ä¢\": \"-\",\n",
    "\"‚Äê\": \"-\",\n",
    "\"-¬≠\": \"-\",\n",
    "\"‚Äú\": \"'\",\n",
    "\"‚Äù\": \"'\",\n",
    "\"&nbsp;\": \" \",\n",
    "\"¬†\": \" \",\n",
    "\"‚Äì\": \"–\",\n",
    "\"¬Æ\": \"®\",\n",
    "\"&amp;\": \"&\",\n",
    "\"‚Äö√Ñ√¨\": \"-\"}\n",
    "def fix(st):\n",
    "    for word, initial in fixer.items():\n",
    "        st = st.replace(word, initial)\n",
    "    return st\n",
    "\n",
    "si, ss, pca = pickle.load(open(\"/data/pbcalder/home77/Documents/hierarchical-hawkes/rnixcnix/lftk/imputer_scaler_pca.p\", \"rb\"))\n",
    "\n",
    "families = [\n",
    "    \"wordsent\",\n",
    "\"partofspeech\",\n",
    "\"entity\",\n",
    "\"avgwordsent\",\n",
    "\"avgpartofspeech\",\n",
    "\"avgentity\",\n",
    "\"lexicalvariation\",\n",
    "\"typetokenratio\",\n",
    "\"readformula\",\n",
    "\"readtimeformula\"]\n",
    "\n",
    "search_feats = []\n",
    "for family in families:\n",
    "    search_feats.extend(lftk.search_features(family = family, return_format = \"list_key\"))\n",
    "    \n",
    "def get_lftk_text_vector(t):\n",
    "    try:\n",
    "        doc = nlp(t)\n",
    "        LFTK = lftk.Extractor(docs = doc)\n",
    "        LFTK.customize(stop_words=True, punctuations=False, round_decimal=3)\n",
    "        extracted_features = LFTK.extract(features=search_feats)#features = selected)\n",
    "        return np.array(list(extracted_features.values()))\n",
    "    except:\n",
    "        return np.full([211], np.nan)\n",
    "\n",
    "def apply_style_pipe(t):\n",
    "    return pca.transform(ss.transform(si.transform(get_lftk_text_vector(t).reshape(1,-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "588882ea-d42e-47db-aa2b-ca323e120dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_df = pd.read_csv('whatif-news.com.au - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaed2b0-ab76-4457-9903-860e7a332098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888f9a6-b637-4213-8dae-2e565b2ab7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0cd7b-f04d-434d-bdb5-4c72d8710e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "array([\"'She was just 13': Albanese's emotional tribute to Bali bombing victims\",\n",
    "       \"Foreign policy and the Albanese government's first 100 days\",\n",
    "       'Stamp duty reform should be allowed to go ahead',\n",
    "       'Stars collide in battle for Jillaroos No.1 jersey at the World Cup',\n",
    "       \"Cryptocurrency company accidentally transfers $10.5m to Australian woman and doesn't notice for seven months\"],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81eed897-6b07-47bc-bb3c-69415e793719",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = pd.read_csv(\"../../../inflammatory/concated.csv\")\n",
    "body_dict, relevant_index = pickle.load(open(\"../../../inflammatory/theta/relevant_bigdf_index.p\", \"rb\"))\n",
    "big_df = big_df.loc[relevant_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f37cb60a-16f1-4f6e-90c0-ba51dd5d51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df['Body'] = big_df['Article id'].map(body_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c098c97c-3b44-4f91-b7a4-90151696c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = pd.concat([\n",
    "    big_df[big_df[\"Headline\"].map(lambda x: 'She was just 13' in x)],\n",
    "    big_df[big_df[\"Headline\"].map(lambda x: 'Foreign policy and the Albanese government' in x)],\n",
    "    big_df[big_df[\"Headline\"].map(lambda x: 'Stamp duty reform should be allowed' in x)],\n",
    "    big_df[big_df[\"Headline\"].map(lambda x: 'Stars collide in battle' in x)],\n",
    "    big_df[big_df[\"Headline\"].map(lambda x: 'Cryptocurrency company accidentally transfers' in x)]\n",
    "], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db1732ad-654a-4589-81d0-349a75b969fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies = [big_df[\"Body\"].values[0]] * 51 + \\\n",
    "[big_df[\"Body\"].values[1]] * 51 + \\\n",
    "[big_df[\"Body\"].values[2]] * 51 + \\\n",
    "[big_df[\"Body\"].values[3]] * 51 + \\\n",
    "[big_df[\"Body\"].values[4]] * 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "72963da3-082a-4582-b63d-1a0b47f2e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_df = relevant_df.rename({\"headline\": \"Headline\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85734d7a-bbc1-4a77-9697-da496c361881",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_df[\"Body\"] = bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f466e91-e52e-42e8-80cf-38b8140205bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_tb = np.hstack([\n",
    "    model.encode(relevant_df[\"Headline\"].values), # title semantic\n",
    "    np.stack(relevant_df[\"Body\"].map(lambda x: apply_style_pipe(x)[0]).values), # body style\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0f4c822-5d4b-4d8f-8e1c-5ed7fae74e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbcalder/anaconda3/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_data = pickle.load(open(\n",
    "    \"/data/pbcalder/rnixcnix-bayes/stan/THETA_0x(m)_0y(t+b)_ATNIX_minE10_maxE50_maxC100_full.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6ccc4f36-8fb8-4258-8e23-194bc2c33bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "dats = []\n",
    "for publisher_index in [16]:#range(28):\n",
    "    logo_data = base_data[int(publisher_index)]\n",
    "    publisher_name = logo_data[0]\n",
    "    publisher_y_scaler = logo_data[1][2]\n",
    "    scaled_vecs = publisher_y_scaler.transform(np.stack(vecs_tb))\n",
    "    dats.append([publisher_index, publisher_name, scaled_vecs])\n",
    "    print(publisher_index)\n",
    "pickle.dump(dats, open(\"scaled_vecs_theta_16.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9a1f5-987b-4ac6-b2c1-9151006a961f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be7d747c-4fe6-4a0c-b03e-4cde7b3ff331",
   "metadata": {},
   "source": [
    "## alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd5888f3-5cba-4a70-9141-a9175d719dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbcalder/anaconda3/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logo_data = pickle.load(open(\"/data/pbcalder/rnixcnix-bayes/stan/LOGO_0x(m)_0y(t)_ATNIX_full.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4f03f99c-db5d-47e0-971a-0dc2babbc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "textvecs = model.encode(relevant_df[\"Headline\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cff4c3e1-2a0a-4d93-b98c-f25359fcf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "dats = []\n",
    "for publisher_idx in [16]:#range(len(logo_data)):\n",
    "    publisher_name = logo_data[publisher_idx][0]\n",
    "    publisher_y_scaler = logo_data[publisher_idx][1][2]\n",
    "    scaled_vecs = publisher_y_scaler.transform(np.stack(textvecs))\n",
    "    dats.append([publisher_idx, publisher_name, scaled_vecs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39ff0aa9-517d-4112-bff4-baf722b44fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dats, open(\"scaled_vecs_16.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fa4405b9-ffc1-472f-8c22-e411f2007157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pbcalder/Documents/analyse_bayesian/inflammatory_rnix/theta/single_analysis\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c27603-5780-4c58-b3bf-cf70fc95a623",
   "metadata": {},
   "source": [
    "# run model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a02a01-f560-4ad6-a076-b7acc4a95129",
   "metadata": {},
   "source": [
    "## theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "16a654ba-b3e8-43fb-b581-7dca73cd33c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of fitted models from FAKENIX\n",
    "model_indices = [16] # news.com.au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "45fbe891-0f99-4700-bb74-3e1ca421f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a, axis=None):\n",
    "    \"\"\"\n",
    "    Computes exp(a)/sumexp(a); relies on scipy logsumexp implementation.\n",
    "    :param a: ndarray/tensor\n",
    "    :param axis: axis to sum over; default (None) sums over everything\n",
    "    \"\"\"\n",
    "    from scipy.special import logsumexp\n",
    "    lse = logsumexp(a, axis=axis)  # this reduces along axis\n",
    "    if axis is not None:\n",
    "        lse = np.expand_dims(lse, axis)  # restore that axis for subtraction\n",
    "    return np.exp(a - lse)\n",
    "\n",
    "\n",
    "def get_hl(log_thet, log_cc):\n",
    "    return log_cc + np.log((2 ** (1 / np.exp(log_thet)) - 1))\n",
    "\n",
    "def get_halftime(x):\n",
    "    if len(x) % 2 == 1:\n",
    "        return (x[(len(x) - 1) // 2])/60/60\n",
    "    else:\n",
    "        return (0.5 * (x[len(x) // 2] + x[(len(x) // 2) - 1]))/60/60\n",
    "\n",
    "def get_average_halftime(l):\n",
    "    return np.median([get_halftime(x) for x in l])\n",
    "\n",
    "def get_halftimes(l):\n",
    "    return [get_halftime(x) for x in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9877a35e-cb6f-4217-9ee7-f8bfe57d51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dats = pickle.load(open(\"scaled_vecs_theta_16.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b279f4a-5b91-4485-b40d-40287de1321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_pvals = []\n",
    "unpop_pvals = []\n",
    "z_pvals = []\n",
    "expected_pvals = []\n",
    "\n",
    "ys_over_models = []\n",
    "for model_index in model_indices:\n",
    "    \n",
    "    ys = [] # log median halflife\n",
    "    vecs_all = dats[0][2]\n",
    "    for selected_test_article_index in range(len(relevant_df)):\n",
    "        scaled_vecs_for_user = vecs_all[selected_test_article_index, :]\n",
    "        sdraws = pd.read_csv(f\"../standraws_with_cov/{model_index}.csv\")\n",
    "\n",
    "        gamma_a_mid = sdraws[[x for x in sdraws.columns if 'gamma_a_plmid[' in x]].values\n",
    "        gamma_a_fast = sdraws[[x for x in sdraws.columns if 'gamma_a_plfast[' in x]].values\n",
    "        gamma_a_slow = sdraws[[x for x in sdraws.columns if 'gamma_a_plslow[' in x]].values\n",
    "        \n",
    "        beta_fast = sdraws[\n",
    "            ['beta_a_plfast[1]', 'beta_a_plfast[2]']].values\n",
    "        \n",
    "        covariance_fast = sdraws[[\n",
    "            'covariance_beta_plfast[1,1]',\n",
    "            'covariance_beta_plfast[2,1]',\n",
    "            'covariance_beta_plfast[1,2]',\n",
    "            'covariance_beta_plfast[2,2]'\n",
    "        ]].values\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        beta_slow = sdraws[\n",
    "            ['beta_a_plslow[1]', 'beta_a_plslow[2]']].values\n",
    "        \n",
    "        covariance_slow = sdraws[[\n",
    "            'covariance_beta_plslow[1,1]',\n",
    "            'covariance_beta_plslow[2,1]',\n",
    "            'covariance_beta_plslow[1,2]',\n",
    "            'covariance_beta_plslow[2,2]'\n",
    "        ]].values\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        beta_mid = sdraws[\n",
    "            ['beta_a_plmid[1]', 'beta_a_plmid[2]']].values\n",
    "        \n",
    "        covariance_mid = sdraws[[\n",
    "            'covariance_beta_plmid[1,1]',\n",
    "            'covariance_beta_plmid[2,1]',\n",
    "            'covariance_beta_plmid[1,2]',\n",
    "            'covariance_beta_plmid[2,2]'\n",
    "        ]].values\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        beta_z = sdraws[\n",
    "            ['beta_a_z[1]', 'beta_a_z[2]']].values\n",
    "        \n",
    "        covariance_z = sdraws[[\n",
    "            'covariance_beta_z[1,1]',\n",
    "            'covariance_beta_z[2,1]',\n",
    "            'covariance_beta_z[1,2]',\n",
    "            'covariance_beta_z[2,2]'\n",
    "        ]].values\n",
    "        \n",
    "        vecs = vecs_all[selected_test_article_index]\n",
    "        \n",
    "        hls = np.zeros(shape=(500, 3))\n",
    "        ps = np.zeros(shape=(500, 3))\n",
    "        for s in range(500):\n",
    "            log_theta_fast, log_c_fast = multivariate_normal(\n",
    "            beta_fast[s],\n",
    "            covariance_fast[s].reshape(2,2))\n",
    "        \n",
    "            log_theta_mid, log_c_mid = multivariate_normal(\n",
    "                beta_mid[s],\n",
    "                covariance_mid[s].reshape(2,2))\n",
    "            \n",
    "            log_theta_slow, log_c_slow = multivariate_normal(\n",
    "                beta_slow[s],\n",
    "                covariance_slow[s].reshape(2,2))\n",
    "            \n",
    "            log_theta_fast += np.dot(scaled_vecs_for_user, gamma_a_fast[s])\n",
    "            log_theta_mid += np.dot(scaled_vecs_for_user, gamma_a_mid[s])\n",
    "            log_theta_slow += np.dot(scaled_vecs_for_user, gamma_a_slow[s])\n",
    "\n",
    "            hl_fast = (get_hl(log_theta_fast, log_c_fast))\n",
    "            hl_mid = (get_hl(log_theta_mid, log_c_mid))\n",
    "            hl_slow = (get_hl(log_theta_slow, log_c_slow))\n",
    "        \n",
    "            hls[s,0] = hl_mid \n",
    "            hls[s,1] = hl_slow \n",
    "            hls[s,2] = hl_fast \n",
    "\n",
    "            beta_z_slow, beta_z_fast = multivariate_normal(\n",
    "                beta_z[s],\n",
    "                covariance_z[s].reshape(2,2))\n",
    "                \n",
    "            p_mid, p_slow, p_fast = softmax([1, beta_z_slow, beta_z_fast])\n",
    "        \n",
    "            ps[s,0] = p_mid \n",
    "            ps[s,1] = p_slow \n",
    "            ps[s,2] = p_fast \n",
    "\n",
    "        \n",
    "        ys.append(np.log(np.median(np.sum(np.exp(hls) * ps, axis=1))))\n",
    "\n",
    "    ys_over_models.append([model_index, ys, relevant_df['class'].values])\n",
    "\n",
    "pickle.dump(ys_over_models, open(\"ys_over_models_theta.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9e315-cf02-47d2-9885-0a66ec9ef4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2377a281-857f-4739-a3ad-f94ed2c65d1c",
   "metadata": {},
   "source": [
    "## alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "87683046-dc8f-406e-949d-70e01d13c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of fitted models from FAKENIX\n",
    "model_indices = [16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "66092dd4-a1ce-48a8-98a8-7877d05ace2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_logit(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "03a5d6a0-a859-4b17-8f51-92688604d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dats = pickle.load(open(\"scaled_vecs_16.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b7ea6783-261b-4af1-9ed4-f1e5a9440296",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_pvals = []\n",
    "unpop_pvals = []\n",
    "z_pvals = []\n",
    "expected_pvals = []\n",
    "\n",
    "ys_over_models = []\n",
    "for model_index in model_indices:\n",
    "    # try:\n",
    "    z_median_list = []\n",
    "    pop_median_list = []\n",
    "    unpop_median_list = []\n",
    "    expected_median_list = []\n",
    "\n",
    "    ys = []\n",
    "    \n",
    "    for selected_test_article_index in range(len(relevant_df)):\n",
    "        vecs_all = dats[0][2]\n",
    "        sdraws = pd.read_csv(f\"../../standraws_with_cov/{model_index}.csv\")\n",
    "        \n",
    "        betas = sdraws[[x for x in sdraws.columns if x.startswith('beta_a')]][:500].values\n",
    "        covariance_betas = sdraws[[x for x in sdraws.columns if 'cov' in x]][:500].values\n",
    "        gammaz = sdraws[[x for x in sdraws.columns if 'gamma_z_alpha' in x]][:500].values\n",
    "        gammaa = sdraws[[x for x in sdraws.columns if 'gamma_a_alpha' in x]][:500].values\n",
    "        \n",
    "        \n",
    "        \n",
    "        vecs = vecs_all[selected_test_article_index]\n",
    "        \n",
    "        zs = []\n",
    "        size_pops = []\n",
    "        size_unpops = []\n",
    "        size_expecteds = []\n",
    "        for s in range(500):\n",
    "            beta_a1, beta_a2, beta_a3 = multivariate_normal(\n",
    "                    betas[s],\n",
    "                    covariance_betas[s].reshape(3,3))\n",
    "        \n",
    "            z_new = inv_logit(beta_a1 + np.dot(gammaz[s], vecs.T))\n",
    "            alpha_popular_new = inv_logit(beta_a2 + np.dot(gammaa[s], vecs.T))\n",
    "            alpha_unpopular_new = inv_logit(beta_a3 + np.dot(gammaa[s], vecs.T))\n",
    "        \n",
    "            size_popular_new = 1 / (1 - alpha_popular_new)\n",
    "            size_unpopular_new = 1 / (1 - alpha_unpopular_new)\n",
    "            expected_size_new = (z_new * size_popular_new + (1-z_new) * size_unpopular_new)\n",
    "        \n",
    "            zs.append(z_new)\n",
    "            size_pops.append(size_popular_new)\n",
    "            size_unpops.append(size_unpopular_new)\n",
    "            size_expecteds.append(expected_size_new)\n",
    "        z_median = np.median(zs)\n",
    "        pop_median = np.median(size_pops)\n",
    "        unpop_median = np.median(size_unpops)\n",
    "        expected_median = np.median(size_expecteds)\n",
    "        \n",
    "        z_median_list.append(z_median)\n",
    "        pop_median_list.append(pop_median)\n",
    "        unpop_median_list.append(unpop_median)\n",
    "        expected_median_list.append(expected_median)\n",
    "\n",
    "        # ys is a list over articles\n",
    "        ys.append(np.log(expected_median))\n",
    "\n",
    "    ys_over_models.append([model_index, ys, relevant_df['class'].values])\n",
    "    # except:\n",
    "    #     ys_over_models.append([model_index,np.nan, np.nan])\n",
    "        \n",
    "pickle.dump(ys_over_models, open(\"ys_over_models_alpha.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1fa3a-687c-4c60-8708-bc22c63709e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fc99011-f6c0-4141-af69-5900f232fb55",
   "metadata": {},
   "source": [
    "# analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "35e37b78-0438-4ec1-83f0-668a96378ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_df = pd.read_csv('whatif-news.com.au - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a7a64d52-37cc-4ac5-8d12-c447dfa9dcd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevant_df['hl_pred'] = pickle.load(open(\"ys_over_models_theta.p\", \"rb\"))[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0bc2b977-6cb1-44bf-8c4b-04cb3afb8d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_df['pop_pred'] = pickle.load(open(\"ys_over_models_alpha.p\", \"rb\"))[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "382f4d03-65f3-4231-8344-19651854f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = []\n",
    "for e in range(1,6):\n",
    "    rel = relevant_df[relevant_df['example'] == e].copy()\n",
    "    rel['hl_%diff'] = (np.exp(rel['hl_pred']) - np.exp(rel['hl_pred'].iloc[0])) / np.exp(rel['hl_pred'].iloc[0])\n",
    "    rel['pop_%diff'] = (np.exp(rel['pop_pred']) - np.exp(rel['pop_pred'].iloc[0])) / np.exp(rel['pop_pred'].iloc[0])\n",
    "    rel['av_%diff'] = 0.5 * (rel['hl_%diff'] + rel['pop_%diff'])\n",
    "    rels.append(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "052a9ec0-ab00-4f6c-a8ee-33b2947aca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_df = pd.concat(rels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e31b468c-e0f9-427d-baa9-9f45c581a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_df.to_csv(\"whatif-filled.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f6172e6e-08d4-4861-b00d-b84308221017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3071124/1150521571.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rel['color'] = rel['class'].map(lambda x: \"black\" if x==\"original\" else \"blue\")\n"
     ]
    }
   ],
   "source": [
    "rel['color'] = rel['class'].map(lambda x: \"black\" if x==\"original\" else \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d937b2d0-1dff-4fa2-829e-ae667892dbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Stars collide in battle for Jillaroos No.1 jersey at the World Cup'\""
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1c68226d-e7c1-4e5b-b1fd-192a3ba63ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbcalder/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:1106: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/pbcalder/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:1106: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/pbcalder/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:1106: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/pbcalder/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:1106: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/pbcalder/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:1106: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,6):\n",
    "    rel = relevant_df[relevant_df['example']==k].copy()\n",
    "    g = sns.jointplot(data=rel, x=\"pop_pred\", y=\"hl_pred\", hue='class')\n",
    "    g.plot_joint(sns.kdeplot, color=\"r\", zorder=0, levels=6)\n",
    "    plt.savefig(f\"example_{k}.png\", bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6468936-a908-4ed6-914a-b33424e319c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
